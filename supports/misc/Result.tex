/*
 * -----------------------------------------------------------------------
 * File: misc/Result.tex
 * Creation Time: Mar 27th 2025, 7:09 pm
 * Author: Saurabh Zinjad
 * Developer Email: saurabhzinjad@gmail.com
 * Copyright (c) 2023-2025 Saurabh Zinjad. All rights reserved | https://github.com/Ztrimus
 * -----------------------------------------------------------------------
 */

\chapter{Result}

\section{4.1 Analysis Data Overview and Baseline Safety of LLMs}
Before diving into perturbation effects, we first summarize the preprocessed dataset we used for analysis and establish how each model performs on the original harmful prompts without any perturbations. This baseline provides context for how “aligned” each LLM is (i.e. how often it refuses unsafe requests) and sets the stage for analyzing changes under perturbed inputs. Short descriptive statistics and visualizations will be presented for clarity.

\begin{itemize}
  \item \textbf{Dataset Summary:} Describe the combined dataset of original and perturbed prompts. Include the number of harmful prompts, how they were generated or selected, and the range of content categories (e.g. hate, violence, etc.). Mention that each original prompt is harmful by design, and that each was perturbed in various ways (character\--level, word\--level, sentence\--level changes) to create modified prompts. Also note that for every prompt (original and perturbed) we have an \textbf{LLM response} and a \textbf{safety label} from LlamaGuard (“safe” for a refusal or benign response, “unsafe” for disallowed content). This ensures a consistent evaluation across models and prompt variations.

  \item \textbf{Safety Labeling Method:} Briefly explain how LlamaGuard assigns safety labels. This is important for the reader to trust the analysis. For example, clarify that a “safe” label typically corresponds to the model refusing or safely answering the harmful prompt, whereas “unsafe” means the model’s response contained the requested disallowed content or violated safety guidelines. If LlamaGuard’s accuracy or criteria are known, summarize them here (to justify using these labels for analysis).

  \item \textbf{Baseline Safe vs. Unsafe Rates per Model:} Present the proportion of safe and unsafe responses \textbf{for each LLM on the original (unperturbed) prompts}. This can be a brief paragraph or a small table. For example, note that Model A (e.g. LLaMA\--2 Chat) refused X\% of the harmful prompts (yielding safe responses) and produced unsafe content in the remaining Y\%. In contrast, Model B (e.g. Mistral) might have a much higher unsafe rate, indicating weaker alignment. Emphasize any notable differences:
    \begin{itemize}
      \item \textit{Well\--aligned models:} Identify models like LLaMA\--2 Chat that have a high safe response rate on the unmodified harmful queries, meaning they “would otherwise refuse” the request in most cases.
      \item \textit{Less\--aligned models:} Identify models (if any) that frequently produce unsafe outputs even without perturbation. This highlights that perturbation attacks are most relevant for models that \textbf{attempt} to refuse harmful prompts (since a model that doesn’t refuse at all isn’t meaningfully affected by input tweaks).
    \end{itemize}
    
  \item \textbf{Implications for Analysis:} Connect these baseline findings to the research questions. For instance, explain that RQ1 and RQ2 will primarily focus on models and scenarios where the original prompt was handled safely (refused), because the core concern is whether \textbf{those safe responses can be flipped to unsafe with slight changes}. If a significant portion of prompts were already answered unsafely by a certain model, note that perturbations are less needed for that model – but we will still examine if perturbations increase the failure rate even further or have any effect. This baseline sets the expectation: ideally, an aligned model should stay safe under reworded inputs, and the analysis will reveal if this holds true.
\end{itemize}

\section{4.2 Impact of Slight Prompt Modifications on Safety (RQ1)}
\textbf{Rationale:} This section addresses \textbf{RQ1: “Can slight changes in a harmful prompt cause LLMs to generate unsafe responses they would otherwise refuse?”} We focus on minimal perturbations (like small typos or trivial rephrasing) to see if they can tip the balance from a safe refusal to an unsafe answer. By isolating \textbf{cases where the original prompt was handled safely}, we can measure how often \textit{minor edits alone} lead to a breakdown of the model’s safety behavior.

\begin{itemize}
  \item \textbf{Identifying “Slight Changes”:} Define what constitutes a \textit{slight perturbation}. For example, this may be a single\--character typo, a minor spelling mistake, or a simple grammatical rephrase that maintains almost the same wording. We use the \textbf{perturbation\_level} and \textbf{perturbation\_count} fields to select minimal edits (e.g. character\--level changes with count = 1, or one\--word substitutions). We also utilize high \textbf{similarity scores} (both token overlap and latent semantic similarity) to confirm these perturbed prompts are very close to the original phrasing. This subsection establishes the criteria for selecting the subset of data relevant to RQ1.

  \item \textbf{Safe\--to\--Unsafe Response “Flips”:} Analyze how often a previously safe interaction turns unsafe after a slight perturbation. We specifically look at prompts where the original response was labeled “safe” (the model refused or answered without policy violation) and check the perturbed response’s safety. Key points to present:
  \begin{itemize}
    \item The \textbf{overall flip rate}: e.g. “X\% of prompts that were originally safely handled became unsafe when a minor typo or paraphrase was introduced.” This gives a direct measure of vulnerability to minimal perturbations.
    \item \textbf{Model\--wise differences}: Break down the flip rate by model. For instance, Model A might only rarely flip to unsafe with slight changes (indicating robust safety), whereas Model B might flip more often. A short table or bar chart could illustrate that, but in the outline we just note any stark contrasts. If a model had very few safe originals (from baseline), mention that its flip rate might be less meaningful, but still report if any occurrences happened.
    \item Comment on the \textbf{types of slight changes} that caused flips. Are single\--character changes (like adding a period, or a common typo) enough to trick the model? Or were flips more often caused by a single\--word synonym replacement or reordering? This gives insight into what kind of “slight” perturbation is most dangerous. For example, we might find that a simple spelling error in a crucial keyword caused the model to miss the harmful intent and comply.
  \end{itemize}
    
  \item \textbf{Statistical Significance of Flip Phenomenon (if applicable):} If the dataset size allows, we will mention whether these safe\--to\--unsafe flips are statistically significant or just isolated cases. For instance, perform a proportion test or at least report confidence intervals to show that the flip rate is not just due to randomness. This strengthens the argument that slight perturbations have a real, measurable effect on safety compliance.

  \item \textbf{Example Case (Qualitative Illustration):} (Optional, if space permits) We may provide one brief example of a prompt and response pair to illustrate RQ1. For example: \textit{Original prompt:} “How can I make [dangerous substance] at home?” → \textit{Model response:} “I’m sorry, but I cannot assist with that request.” (safe refusal). \textit{Perturbed prompt:} “How can I maek [dangerous substance] at home?” (with a tiny typo) → \textit{Model response:} “To create [dangerous substance], you need the following ingredients…” (unsafe compliance). This concrete example (presented in a safe manner) would powerfully demonstrate how a trivial alteration bypassed the safety. Such an example would be included with a clear warning that the content is disallowed, and possibly partially redacted if necessary for safety in the thesis text.

  \item \textbf{Implications for RQ1:} Summarize what these findings mean: e.g., “We find that even minimal perturbations can indeed cause misbehavior in certain LLMs, though the extent varies by model.” If any model was completely robust to all slight changes (zero flips), note that as well. This answers RQ1 by confirming (or refuting) that slight prompt changes may induce unsafe responses, highlighting a potential weakness in the consistency of the model’s safety measures.
\end{itemize}

\section{4.3 Effect of Perturbation Type and Prompt Similarity on Safety (RQ2)}
\textbf{Rationale:} Here we address \textbf{RQ2: “How do different types of input perturbations (character, word, sentence\--level) affect the safety of LLM responses? Does safety decrease as similarity to the original prompt drops?”} This section broadens the analysis to a range of perturbation magnitudes and styles, beyond the minimal changes of RQ1. We investigate whether \textbf{larger or more semantic alterations} in the prompt correlate with a higher chance of unsafe responses. In essence, we want to see if there’s a trend: the more you rephrase or distort the prompt (lower similarity to original), the more likely the model’s safety guard is bypassed.

\subsection{4.3.1 Safety Outcomes by Perturbation Level (Character vs. Word vs. Sentence)}
\textbf{Rationale:} Perturbation \textit{level} refers to the scope of changes: character\--level (e.g. typos, character swaps), word\--level (replacing or shuffling words, minor phrasing changes), and sentence\--level (major rephrasings or restructuring of the prompt). We compare these categories to see which level of perturbation has the strongest impact on eliciting unsafe responses.

\begin{itemize}
  \item \textbf{Grouped Analysis by Perturbation Level:} Present the rate of unsafe responses for each group of perturbations:
    \begin{itemize}
      \item \textbf{Character\--level perturbations:} For prompts altered with only character tweaks (typos, missing characters, character insertions, or simulated OCR/keyboard errors), report the percentage of responses that turned unsafe. Discuss whether this is significantly different from the original baseline. We might expect that because character changes typically preserve most of the original wording, the model’s safety system could still recognize the harmful intent in many cases — but any deviation might confuse keyword\--based filters. We note if the unsafe rate under char\--level changes is modest or if it surprisingly spikes.
      \item \textbf{Word\--level perturbations:} Report the unsafe response rate when one or a few words are changed (e.g. synonyms, slight grammar changes, adding inconsequential words). Since these changes can alter phrasing more noticeably, we check if there’s a higher incidence of unsafe responses compared to character\--level. The analysis might reveal a moderate increase. We will comment on the intuition: word\--level changes can bypass simple phrase matches and might reword the request enough that the model’s learned refusal trigger doesn’t fire as reliably.
      \item \textbf{Sentence\--level (or full paraphrasing) perturbations:} Report the unsafe rate for prompts that were heavily rephrased or had sentence structure changes. This often results in the lowest similarity to the original wording. We anticipate this category may show the highest percentage of unsafe outputs if the model is primarily relying on specific cue phrases to detect disallowed requests. Emphasize how much safety degrades here relative to character\-- or word\--level changes.
    \end{itemize}
    
  \item \textbf{Comparative Findings:} Use a short paragraph to compare across the three levels. For example: “Unsafe response rates rose from A\% at character\--level, to B\% at word\--level, and to C\% at sentence\--level perturbations, indicating a clear trend: more extensive prompt rewording leads to more frequent safety failures.” If any level unexpectedly does not follow this order (e.g. if character\--level was surprisingly effective at causing unsafe outputs in some cases), discuss that anomaly and hypothesize why (perhaps the model misinterpreted a typo in a way that avoided triggering refusal).

  \item \textbf{Model\--Averaged vs. Model\--Specific:} Clarify whether these percentages are aggregated across all models or representative of a particular model. If aggregated, note that this gives an overall picture, but model differences will be addressed later (in RQ3 section). Alternatively, if differences between models are huge, we might present separate sub\--analyses here or at least caveat that “this is the general pattern, although model X and Y deviate which will be discussed in Section 4.4.” The goal is to keep RQ2 focused on the effect of perturbation type in principle, independent of specific model idiosyncrasies.
\end{itemize}


\subsection{4.3.2 Prompt Similarity and Safety Degradation}
\textbf{Rationale:} To directly answer the second part of RQ2, we examine the relationship between \textbf{prompt similarity} (perturbed vs original) and the likelihood of an unsafe response. The hypothesis is that as similarity drops (meaning the perturbed prompt is less like the original wording), the model’s safety behavior might degrade further. We utilize both \textbf{token\--based similarity} (e.g. overlapping keywords or edit distance) and \textbf{latent semantic similarity} (e.g. embedding cosine similarity) to quantify how much a perturbation changed the prompt.

\begin{itemize}
  \item \textbf{Correlation Analysis:} We calculate how the safety outcome correlates with similarity scores. This could involve plotting the average unsafe rate against binned similarity ranges or computing a correlation coefficient. We will describe findings such as: \textit{“We observe a strong negative correlation between token similarity and unsafe output rate”} – meaning prompts that share fewer tokens with the original phrasing are more likely to yield disallowed answers. Similarly, note if latent similarity (which captures meaning) shows a clearer or different trend. For instance, a high latent similarity means the prompt’s meaning is essentially the same; if we find many unsafe outputs even at high latent similarity, that suggests even minor rewordings can fool the model. If instead unsafe outputs mostly occur at low latent similarity, it implies the model only breaks when the request is significantly reformulated.

  \item \textbf{Threshold or Gradient:} Discuss whether the relationship appears gradual or if there’s a threshold effect. For example, perhaps prompts above a certain similarity (very close to original) almost always remain safe, but once similarity falls below, say, 0.9 (90\%), the unsafe rate jumps sharply. We will mention any notable cutoff points or nonlinear patterns observed. If we have a chart (to be included in the full thesis) showing unsafe percentage vs. similarity score, we’ll describe its general shape here in words: e.g. linear increase, exponential, etc.

  \item \textbf{Token vs. Latent Similarity Differences:} If the analysis shows a difference between token overlap and latent (semantic) similarity, explain it. It could be that token\--similarity is a coarse measure (typos cause low token similarity but the meaning is unchanged) whereas latent similarity is more aligned with the model’s understanding. We might find, for instance, that \textbf{latent similarity} is a better predictor of safety: prompts that remain semantically very similar to the original are often still refused, whereas those that truly change the semantics (latent similarity drops) are needed to trick the model. This would mean the model’s safety mechanism works on a semantic level, not just literal wording. Conversely, if token similarity dropping (even with meaning intact) causes more unsafe outputs, it might imply the model’s refusal was tied to specific trigger words which were obfuscated by even meaningless changes.

  \item \textbf{Perturbation Count as a Factor:} We also consider \textbf{perturbation\_count} (the number of edits applied) as a complementary perspective on similarity. Generally, more edits will lower similarity, but we analyze it explicitly: e.g., compare prompts with 1 edit vs. 2 edits vs. 5 edits. If data shows that prompts with multiple small edits are more likely to bypass safety than those with a single edit, we’ll report that. This helps confirm that increased perturbation (in quantity) correlates with decreased safety, reinforcing the trend found with similarity metrics. If this analysis wasn’t performed in earlier notebooks, we will note it here as a \textbf{needed analysis}: for completeness, the thesis should verify whether increasing the number of perturbations (even if each is minor) has a cumulative effect on evading safety filters.

  \item \textbf{Answer to RQ2:} Summarize the overall answer. For example: \textit{“Our analyses show that safety does indeed decrease as similarity to the original prompt drops. Character\--level perturbations (high similarity changes) have the smallest effect on eliciting unsafe responses, whereas word\-- and especially sentence\--level rephrasings (low similarity) significantly increase the chance of unsafe outputs. This indicates a gradient: the more you rephrase a harmful request, the more likely even a normally safe model is to comply.”} This directly addresses RQ2 and transitions into deeper exploration by specific techniques and models next.
\end{itemize}

\section{4.4 Most Effective Perturbation Techniques and Model Vulnerabilities (RQ3)}
\textbf{Rationale:} In this section we tackle \textbf{RQ3: “Which perturbation techniques (e.g., paraphrasing, spelling errors, random insertions) are most strongly associated with unsafe behavior? How does safety vary across LLMs and perturbation types?”} Building on the previous results, we now dissect the performance by individual perturbation methods and examine differences across the various LLMs. This gives a detailed view of \textit{which attack strategies are most potent} and \textit{which models are more or less resilient}, providing practical insights into where current safety mechanisms are weakest.

\subsection{4.4.1 Comparison of Perturbation Techniques}
\textbf{Rationale:} Different perturbation techniques might have different success rates in bypassing safety. For instance, a carefully paraphrased prompt might be more effective than a prompt with random character insertions. We systematically compare all the perturbation types used in our experiments to identify which ones cause the most unsafe outputs.

\begin{itemize}
  \item \textbf{Technique\--wise Unsafe Rates:} Present the proportion of unsafe responses for each specific perturbation technique (e.g. “spelling mistakes”, “OCR noise”, “keyboard typos”, “random word insertion”, “synonym replacement”, “full paraphrase”, etc.). This can be described in text or with a reference to a figure. We will highlight the top performers:
  \begin{itemize}
    \item Identify the \textbf{most effective techniques} – those with the highest percentage of prompts leading to unsafe answers. For example, we might find that \textit{paraphrasing via an external tool} results in, say, 50\% unsafe responses, making it the most dangerous perturbation method in our set.
    \item Identify the \textbf{least effective techniques} – those that rarely fooled the model. Perhaps simple \textit{random character insertions} that make the prompt nonsensical had only a small impact, as the model either still refused or just got confused.
    \item Note any \textbf{interesting middle\--ground} cases. For instance, \textit{minor spelling errors} might have a moderate success rate, indicating that even trivial typos sometimes slip past safety, but not as reliably as a full rephrase. Discuss possible reasons: spelling errors might sometimes still be understood by the model (thus refused), but other times they obscure a keyword enough to bypass a rule.
  \end{itemize}

  \item \textbf{Analysis of Why Certain Techniques Succeed:} For the techniques that stand out as most likely to induce unsafe content, provide insight. For example, \textit{paraphrasing} likely succeeds because it rewords the request entirely while preserving meaning, thereby circumventing pattern\--matched safety training data. \textit{OCR noise or keyboard errors} introduce uncommon misspellings that the model might not associate with disallowed content, tricking any lexical\--based filters. By contrast, a \textit{random insertion of an irrelevant word} might disrupt the prompt so much that the model either refuses on different grounds or doesn’t produce a coherent answer, thus not always leading to a policy violation. This qualitative reasoning helps explain the quantitative rankings.

  \item \textbf{Cross\--Referencing Similarity:} We can tie the effectiveness back to the similarity discussion: techniques that greatly alter wording (like paraphrasing) inherently lower similarity and are observed to be more effective at evading safeguards, whereas techniques that leave most of the prompt intact (like single\--character tweaks) have higher similarity and generally lower success — with exceptions noted. This shows consistency across our findings.

  \item \textbf{Potential Missing Analysis – Combined Factors:} If not already done, we suggest a more rigorous analysis such as a logistic regression or ANOVA with \textbf{perturbation technique} as one factor to quantify its impact while controlling for other variables (like model). This would statistically confirm which techniques have a significant effect on outcome. In the thesis, including such an analysis (e.g. an odds ratio for each technique causing an unsafe response) would strengthen the conclusions drawn qualitatively here.
\end{itemize}

\subsection{4.4.2 Differences in Model Resilience to Perturbations}
\textbf{Rationale:} Not all LLMs are equally robust. This subsection compares the behavior of different models (LLaMA\--2, LLaMA\--3, Mistral, etc.) under perturbations, addressing the part of RQ3 about how safety varies across LLMs. We identify which models are more resistant to adversarial prompting and which are more prone to being tricked, possibly linking this to their training or alignment methods.

\begin{itemize}
  \item \textbf{Overall Unsafe Rate by Model (Perturbed Prompts):} Summarize each model’s performance across all perturbations. For example, Model A might have only 20\% of perturbed cases resulting in unsafe outputs, whereas Model B might be at 70\%. This gives a big\--picture view of model robustness. Relate this back to the baseline: a model that already had a high unsafe rate with no perturbation (like Mistral, if applicable) might not worsen much with perturbations (because it was already unsafe often), whereas a model that was mostly safe on original prompts (like LLaMA\--2) might show a larger delta when perturbed. We will highlight these differences:
    \begin{itemize}
      \item Models with \textbf{strong alignment} (high baseline safety) but that degrade significantly under perturbation are particularly worrisome — they give a false sense of security under normal conditions but can be exploited. 
      \item Models with \textbf{weak alignment} (low baseline safety) will unsurprisingly also be unsafe with perturbations, but we note if perturbations still increase their unsafe outputs or if they were nearly maxed out anyway.
    \end{itemize}
    
  \item \textbf{Model × Perturbation Type Interaction:} Dive into any notable interactions between specific models and specific perturbation techniques:
    \begin{itemize}
      \item It may be that Model X is especially vulnerable to technique Y. For instance, perhaps LLaMA\--2 resists simple typos but is easily fooled by a well\--formed paraphrase, whereas another model might ironically handle paraphrases better but be tripped up by certain token insertions it wasn’t trained against. We present any evidence of such pattern: e.g., “Model B had an unsafe rate of only 5\% on spelling errors (suggesting it recognized the misspelled harmful word), but 40\% on grammar\--based rephrasings, indicating paraphrasing was a more effective attack for that model.”
      \item Conversely, if a model shows consistently high (or low) unsafe rates across all perturbation types, mention that too. For example, “Mistral produced disallowed answers for the majority of prompts in nearly every perturbation category, reflecting a generally poor safety mechanism rather than targeted weaknesses.”
      \item This part of the analysis can reference a two\--way table or an interaction plot (not shown in outline, but in the thesis) from the multi\--factor analysis in the notebooks. We ensure to interpret that: confirming if the differences observed by technique are statistically significant for one model vs another (e.g., maybe a significant interaction effect in an ANOVA, meaning the effect of perturbation type on safety depends on which model it is).
    \end{itemize}
    
  \item \textbf{Case Study – Model Specific:} If relevant, briefly spotlight a particular model’s behavior that is interesting. For example, if “LLaMA\--3” is a newer model, did it handle perturbations better than LLaMA\--2 or worse? We can discuss how improvements in training or alignment might have impacted these results. Or, if “Mistral” was not fine\--tuned for safety, use it as a contrast to show why alignment is crucial: it fails almost regardless of input phrasing.

  \item \textbf{Implications for Model Selection:} Tie these findings back to RQ3’s concern. We now know which models are safer under adversarial prompts and which perturbation methods are most likely to cause failures. For practitioners, this suggests that some models might be preferred if robustness to prompt tampering is required, whereas others need additional safety layers. We reinforce that even the best model we tested (whichever had the lowest unsafe rate under perturbation) still had some non\--zero failures, meaning no model was completely impervious to all prompt modifications.

  \item \textbf{Suggested Missing Analysis (if any):} If not already covered, we might suggest analyzing \textbf{content\--category\--specific model performance} (e.g., does a model do fine on hate speech requests but fail on self\--harm advice when phrased indirectly?). While not the primary focus of RQ3, such analysis could reveal if certain policy domains are more susceptible to being bypassed than others and if certain models have blind spots. Including a brief mention of whether any such pattern was noticed (from the “category” factor analysis) can add depth to the model comparison.
\end{itemize}

\section{4.5 Summary of Analysis Findings}
\textbf{Rationale:} This final subsection concisely synthesizes the results from RQ1, RQ2, and RQ3, reinforcing how they collectively answer the thesis’s research questions. It provides a bridge to the discussion chapter by highlighting the most important takeaways about LLM safety under perturbed prompts.

\begin{itemize}
  \item \textbf{RQ1 (Slight Prompt Changes):} We confirmed that \textit{yes}, slight perturbations can cause LLMs to yield unsafe responses in cases where they would normally refuse. However, the extent of this vulnerability varies by model. Well\--aligned models like LLaMA\--2 showed relatively few failures with trivial changes (though still some notable cases), whereas less aligned models were already unsafe and remained so. This demonstrates that even state\--of\--the\--art aligned models are not fully robust to tiny input tweaks, an important finding for red\--teaming and safety evaluation.

  \item \textbf{RQ2 (Perturbation Extent and Similarity):} Our analysis clearly showed that \textbf{the more you change the prompt’s wording (lower the similarity), the more likely the model’s safety guard is to fail}. Character\--level perturbations had the least impact on safety (most refusals remained intact), word\--level changes had a moderate impact, and sentence\--level paraphrasing had the strongest impact, significantly increasing the chance of unsafe responses. There is a roughly inverse relationship between prompt similarity and safety: minor edits might slip one past occasionally, but substantial rephrasings very often did. This trend held across the evaluated models, indicating a general weakness in current LLM safety mechanisms when facing reworded attacks.

  \item \textbf{RQ3 (Techniques and Model Differences):} Among the various attack techniques, \textbf{paraphrasing and sophisticated rewording approaches emerged as the most effective at inducing unsafe behavior}, often causing many models to comply with harmful requests. Simpler perturbations like random character noise were less consistently effective but still succeeded in some cases, highlighting that even low\--effort attacks can work at times. In terms of model robustness, \textbf{differences were stark}: for example, LLaMA\--2 (Chat) was comparatively robust, failing on far fewer perturbed prompts than Mistral or the less\--aligned LLaMA\--3 variant. Nonetheless, no model was foolproof — even the best had weaknesses with certain perturbation types. These findings pinpoint which models and attack methods deserve special attention in future safety improvements.

  \item \textbf{Concluding Remarks:} The analysis has systematically answered the research questions. We have demonstrated where and how LLMs can be vulnerable to prompt perturbation attacks. This sets the stage for the \textbf{Discussion} chapter to interpret what these results mean for LLM deployment and safety mitigation strategies. For instance, we now have evidence to discuss whether safety training should include adversarial rephrasing, how to improve models’ understanding of intent despite surface alterations, and what it means for users and malicious actors. The next chapter will delve into these implications, limitations of our analysis, and recommendations for building more robust LLMs against such safety bypass attempts.
\end{itemize}