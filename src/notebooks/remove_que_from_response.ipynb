{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the directory containing the datasets\n",
    "data_dir = \"../../data/\"\n",
    "dataset_path = os.path.join(data_dir, \"analyzed/catHarmQA/combined_catqa.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv(dataset_path)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['category', 'subcategory', 'original_question',\n",
       "       'original_question_safety', 'original_response',\n",
       "       'original_response_safety', 'original_response_pre',\n",
       "       'original_response_pre_safety', 'experiment', 'perturbation_level',\n",
       "       'perturbation_type', 'perturbation_count', 'perturbed_question',\n",
       "       'perturbed_question_safety', 'model', 'perturbed_response',\n",
       "       'perturbed_response_safety', 'perturbed_response_pre',\n",
       "       'perturbed_response_pre_safety', 'latent_similarity',\n",
       "       'token_similarity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text in \"orignal_response\" column start with question which from \"original_question\" columns and It's not needed, so we have to remove that question. so, we check if question from \"original_question\" match with start portion of \"orignal_response\" then remove that question from \"orignal_response\" column. same will happen with \"perturbed_response\" column which have question from \"perturbed_question\" column. remember that data variable is pandas dataframe and we can access columns by their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_eage_case(content):\n",
    "    # edge cases\n",
    "    for condition in [\" ' \", \" '\", \" !\", \" ,\", \" ?\", \" .\", \" '\"]:\n",
    "        if condition in content:\n",
    "            content = content.replace(condition, condition.strip())\n",
    "        # content = re.sub(r\"\\s*'\\s*\", \"'\", content)\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "# Function to remove question from response\n",
    "def remove_question_from_response(row, question_col, response_col, handle_edge_case_condition=False):\n",
    "    question = row[question_col] if not handle_edge_case_condition else handle_eage_case(row[question_col])\n",
    "    response = row[response_col] if not handle_edge_case_condition else handle_eage_case(row[response_col])\n",
    "    if response.startswith(question):\n",
    "        return response[len(question) :].strip()\n",
    "    else:\n",
    "        if handle_edge_case_condition and not response.startswith(question):\n",
    "            print(f\"{row.name}th row\")\n",
    "            print(f\"question: {question}\")\n",
    "            print(f\"response: {response}\", f\"\\n{'-'*120}\\n\")      \n",
    "            return response\n",
    "        else:\n",
    "            return remove_question_from_response(row, question_col, response_col, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to both original and perturbed columns\n",
    "data[\"original_response_pre\"] = data.apply(\n",
    "    lambda row: remove_question_from_response(\n",
    "        row, \"original_question\", \"original_response\"\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "data[\"perturbed_response_pre\"] = data.apply(\n",
    "    lambda row: remove_question_from_response(\n",
    "        row, \"perturbed_question\", \"perturbed_response\"\n",
    "    ),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test if prefix question is removed from \"original_response\" and \"perturbed_response\" columns. we will do that by coding. we will take difference between \"original_response_pre\" and \"original_response\", it will output text from \"original_response\" which is not in \"original_response_pre\" and different will be equal to \"original_question\". so perform this check over all rows and return count of False. we will do same for \"perturbed_response_pre\" and \"perturbed_response\" columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_non_common_substring(A, B):\n",
    "    index = A.find(B)\n",
    "    if index == -1:\n",
    "        return A\n",
    "    else:\n",
    "        return A[:index]\n",
    "\n",
    "# Function to check if the prefix question is removed\n",
    "def check_prefix_removed(row, question_col, response_pre_col, response_col, handle_edge_case_condition=False):\n",
    "    question = row[question_col] if not handle_edge_case_condition else handle_eage_case(row[question_col])\n",
    "    response = row[response_col] if not handle_edge_case_condition else handle_eage_case(row[response_col])\n",
    "    response_pre = row[response_pre_col]\n",
    "\n",
    "    non_common_substring = find_non_common_substring(response, response_pre).strip()\n",
    "\n",
    "    if non_common_substring.strip() == question.strip():\n",
    "        return True\n",
    "    else:\n",
    "        if handle_edge_case_condition and non_common_substring.strip() != question.strip():\n",
    "            # print(f\"{row.name}th row\")\n",
    "            # print(f\"question: {question}\")\n",
    "            # print(f\"response: {response}\")\n",
    "            # print(f\"resp_pre: {response_pre}\", f\"\\n{'-'*120}\\n\")\n",
    "            return question.__contains__(non_common_substring)\n",
    "        else:\n",
    "            return check_prefix_removed(row, question_col, response_pre_col, response_col, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the check function to both original and perturbed columns\n",
    "original_check = data.apply(\n",
    "    lambda row: check_prefix_removed(\n",
    "        row, \"original_question\", \"original_response_pre\", \"original_response\"\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "perturbed_check = data.apply(\n",
    "    lambda row: check_prefix_removed(\n",
    "        row, \"perturbed_question\", \"perturbed_response_pre\", \"perturbed_response\"\n",
    "    ),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check and Validation of above Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if all rows satisfy `original_question + original_response_pre == original_response`\n",
    "original_check.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if all rows satisfy `perturbed_question + perturbed_response_pre == perturbed_response`\n",
    "perturbed_check.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of False for perturbed_response: 0\n"
     ]
    }
   ],
   "source": [
    "perturbed_false_count = len(perturbed_check) - perturbed_check.sum()\n",
    "print(f\"Count of False for perturbed_response: {perturbed_false_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: bool)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturbed_check[perturbed_check == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perturbed_response_pre == perturbed_response: \n",
      "Series([], Name: count, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"perturbed_response_pre == perturbed_response: \\n{perturbed_check[perturbed_check == False].index.to_series().apply(lambda x: data.loc[x, \"perturbed_response_pre\"] == data.loc[x, \"perturbed_response\"]).value_counts()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: count, dtype: int64)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturbed_check[perturbed_check == False].index.to_series().apply(lambda x: data.loc[x, \"model\"]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in perturbed_check[perturbed_check == False].index:\n",
    "    print(\"------ \", data.loc[index, \"perturbed_question\"], \"\\n\")\n",
    "    print(\"------ \", data.loc[index, \"perturbed_response\"], \"\\n\")\n",
    "    print(\"------ \", data.loc[index, \"perturbed_response_pre\"], \"\\n\")\n",
    "    print(data.loc[index, \"perturbed_response_pre\"] == data.loc[index, \"perturbed_response\"], \"\\n\")\n",
    "    print(f\"{'-'*120}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_csv(dataset_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess safety response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path_1 = os.path.join(data_dir, \"analyzed/catHarmQA/combined_catqa1.csv\")\n",
    "dataset_path_2 = os.path.join(data_dir, \"analyzed/catHarmQA/combined_catqa2.csv\")\n",
    "\n",
    "data1 = pd.read_csv(dataset_path_1)\n",
    "data2 = pd.read_csv(dataset_path_2)\n",
    "\n",
    "# for column in ['category', 'subcategory', 'original_question', 'original_question_safety', 'original_response', 'original_response_safety', 'perturbation_level', 'perturbation_type', 'perturbed_question', 'perturbed_question_safety', 'model', 'perturbed_response', 'perturbed_response_safety', 'experiment', 'original_response_pre', 'original_response_pre_safety', 'perturbed_response_pre', 'perturbed_response_pre_safety']:\n",
    "#     data1[column] = data1[column].astype('string')\n",
    "#     data2[column] = data2[column].astype('string')\n",
    "\n",
    "# data1.to_csv(dataset_path_1, index=False)\n",
    "# data2.to_csv(dataset_path_2, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis - After removing questions from response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['category', 'subcategory', 'original_question',\n",
       "       'original_question_safety', 'original_response',\n",
       "       'original_response_safety', 'perturbation_level', 'perturbation_type',\n",
       "       'perturbation_count', 'perturbed_question', 'perturbed_question_safety',\n",
       "       'model', 'perturbed_response', 'perturbed_response_safety',\n",
       "       'experiment', 'original_response_pre', 'perturbed_response_pre'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.to_csv(dataset_path_2, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "perturbed_response_pre_safety\n",
       "safe      92586\n",
       "unsafe    43814\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.perturbed_response_pre_safety.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of changed rows: 59206\n"
     ]
    }
   ],
   "source": [
    "changed_rows = data[\n",
    "    data[\"perturbed_response_safety\"] != data[\"perturbed_response_pre_safety\"]\n",
    "]\n",
    "num_changed = changed_rows.shape[0]\n",
    "print(f\"Number of changed rows: {num_changed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of changed rows: 43.41%\n"
     ]
    }
   ],
   "source": [
    "percentage_changed = (num_changed / len(data)) * 100\n",
    "print(f\"Percentage of changed rows: {percentage_changed:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "original_question_safety\n",
       "unsafe    0.932727\n",
       "safe      0.067273\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.original_question_safety.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', '+', '5', '/', '2']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m for m in list(\" 3+5 / 2 \") if m != ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "<>:6: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "<>:8: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "<>:10: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "<>:4: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "<>:6: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "<>:8: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "<>:10: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "/var/folders/4s/_ygj2ww537qg94w4rftspr4w0000gn/T/ipykernel_41381/3121466902.py:4: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "  if operator is '*':\n",
      "/var/folders/4s/_ygj2ww537qg94w4rftspr4w0000gn/T/ipykernel_41381/3121466902.py:6: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "  elif operator is '/':\n",
      "/var/folders/4s/_ygj2ww537qg94w4rftspr4w0000gn/T/ipykernel_41381/3121466902.py:8: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "  if operator is '+':\n",
      "/var/folders/4s/_ygj2ww537qg94w4rftspr4w0000gn/T/ipykernel_41381/3121466902.py:10: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "  if operator is '-':\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m             s_list\u001b[38;5;241m.\u001b[39mpop(i)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m s_list\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m---> 29\u001b[0m \u001b[43mcalculate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m3+2/2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m, in \u001b[0;36mcalculate\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     14\u001b[0m s_list \u001b[38;5;241m=\u001b[39m [char \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(s) \u001b[38;5;28;01mif\u001b[39;00m char \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(s_list)):\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43ms_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     17\u001b[0m         s_list[i] \u001b[38;5;241m=\u001b[39m operation(s_list[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], s_list[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m], s_list[i])\n\u001b[1;32m     18\u001b[0m         s_list\u001b[38;5;241m.\u001b[39mpop(i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def operation(a, b, operator):\n",
    "        a = int(a)\n",
    "        b = int(b)\n",
    "        if operator is '*':\n",
    "            return a * b\n",
    "        elif operator is '/':\n",
    "            return a / b\n",
    "        if operator is '+':\n",
    "            return a + b\n",
    "        if operator is '-':\n",
    "            return a - b\n",
    "\n",
    "def calculate(s: str) -> int:\n",
    "    s_list = [char for char in list(s) if char != ' ']\n",
    "    for i in range(len(s_list)):\n",
    "        if s_list[i] in ['*', '/']:\n",
    "            s_list[i] = operation(s_list[i-1], s_list[i+1], s_list[i])\n",
    "            s_list.pop(i-1)\n",
    "            s_list.pop(i)\n",
    "\n",
    "    for i in range(len(s_list)):\n",
    "        if s_list[i] in ['+', '-']:\n",
    "            s_list[i] = operation(s_list[i-1], s_list[i+1], s_list[i])\n",
    "            s_list.pop(i-1)\n",
    "            s_list.pop(i)\n",
    "\n",
    "    return s_list.pop()\n",
    "\n",
    "calculate(\"3+2/2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['category', 'subcategory', 'original_question',\n",
       "       'original_question_safety', 'original_response',\n",
       "       'original_response_safety', 'original_response_pre',\n",
       "       'original_response_pre_safety', 'experiment', 'perturbation_level',\n",
       "       'perturbation_type', 'perturbation_count', 'perturbed_question',\n",
       "       'perturbed_question_safety', 'model', 'perturbed_response',\n",
       "       'perturbed_response_safety', 'perturbed_response_pre',\n",
       "       'perturbed_response_pre_safety', 'latent_similarity',\n",
       "       'token_similarity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "original_response_pre_safety\n",
       "safe      0.64\n",
       "unsafe    0.36\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.original_response_pre_safety.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "perturbed_response_pre_safety\n",
       "safe      0.679289\n",
       "unsafe    0.320711\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.perturbed_response_pre_safety.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = data[data.original_response_pre_safety == 'unsafe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = ss[ss.perturbed_response_pre_safety == 'safe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24330, 21)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1783724340175953"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.shape[0] / data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_path = \"/Users/saurabh/AA/convergent/projects/llm-sensitivity/data/original/xtest/xstest_prompts.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.read_csv(x_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'prompt', 'type', 'label', 'focus', 'note'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "safe      250\n",
       "unsafe    200\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_safe = dd[dd.label == 'safe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_safe.to_csv(\n",
    "    \"/Users/saurabh/AA/convergent/projects/llm-sensitivity/data/original/xtest/xstest_prompts_safe.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
