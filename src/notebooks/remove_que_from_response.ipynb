{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136400, 17)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the directory containing the datasets\n",
    "data_dir = \"../../data/\"\n",
    "dataset_path = os.path.join(data_dir, \"analyzed/catHarmQA/combined_catqa.csv\")\n",
    "\n",
    "data = pd.read_csv(dataset_path)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['category', 'subcategory', 'original_question',\n",
       "       'original_question_safety', 'original_response',\n",
       "       'original_response_safety', 'perturbation_level', 'perturbation_type',\n",
       "       'perturbation_count', 'perturbed_question', 'perturbed_question_safety',\n",
       "       'model', 'perturbed_response', 'perturbed_response_safety',\n",
       "       'experiment', 'original_response_pre', 'perturbed_response_pre'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text in \"orignal_response\" column start with question which from \"original_question\" columns and It's not needed, so we have to remove that question. so, we check if question from \"original_question\" match with start portion of \"orignal_response\" then remove that question from \"orignal_response\" column. same will happen with \"perturbed_response\" column which have question from \"perturbed_question\" column. remember that data variable is pandas dataframe and we can access columns by their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_eage_case(content):\n",
    "    # edge cases\n",
    "    for condition in [\" ' \", \" '\", \" !\", \" ,\", \" ?\", \" .\", \" '\"]:\n",
    "        if condition in content:\n",
    "            content = content.replace(condition, condition.strip())\n",
    "        # content = re.sub(r\"\\s*'\\s*\", \"'\", content)\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "# Function to remove question from response\n",
    "def remove_question_from_response(row, question_col, response_col, handle_edge_case_condition=False):\n",
    "    question = row[question_col] if not handle_edge_case_condition else handle_eage_case(row[question_col])\n",
    "    response = row[response_col] if not handle_edge_case_condition else handle_eage_case(row[response_col])\n",
    "    if response.startswith(question):\n",
    "        return response[len(question) :].strip()\n",
    "    else:\n",
    "        if handle_edge_case_condition and not response.startswith(question):\n",
    "            print(f\"{row.name}th row\")\n",
    "            print(f\"question: {question}\")\n",
    "            print(f\"response: {response}\", f\"\\n{'-'*120}\\n\")      \n",
    "            return response\n",
    "        else:\n",
    "            return remove_question_from_response(row, question_col, response_col, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to both original and perturbed columns\n",
    "data[\"original_response_pre\"] = data.apply(\n",
    "    lambda row: remove_question_from_response(\n",
    "        row, \"original_question\", \"original_response\"\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "data[\"perturbed_response_pre\"] = data.apply(\n",
    "    lambda row: remove_question_from_response(\n",
    "        row, \"perturbed_question\", \"perturbed_response\"\n",
    "    ),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test if prefix question is removed from \"original_response\" and \"perturbed_response\" columns. we will do that by coding. we will take difference between \"original_response_pre\" and \"original_response\", it will output text from \"original_response\" which is not in \"original_response_pre\" and different will be equal to \"original_question\". so perform this check over all rows and return count of False. we will do same for \"perturbed_response_pre\" and \"perturbed_response\" columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_non_common_substring(A, B):\n",
    "    index = A.find(B)\n",
    "    if index == -1:\n",
    "        return A\n",
    "    else:\n",
    "        return A[:index]\n",
    "\n",
    "# Function to check if the prefix question is removed\n",
    "def check_prefix_removed(row, question_col, response_pre_col, response_col, handle_edge_case_condition=False):\n",
    "    question = row[question_col] if not handle_edge_case_condition else handle_eage_case(row[question_col])\n",
    "    response = row[response_col] if not handle_edge_case_condition else handle_eage_case(row[response_col])\n",
    "    response_pre = row[response_pre_col]\n",
    "\n",
    "    non_common_substring = find_non_common_substring(response, response_pre).strip()\n",
    "\n",
    "    if non_common_substring.strip() == question.strip():\n",
    "        return True\n",
    "    else:\n",
    "        if handle_edge_case_condition and non_common_substring.strip() != question.strip():\n",
    "            # print(f\"{row.name}th row\")\n",
    "            # print(f\"question: {question}\")\n",
    "            # print(f\"response: {response}\")\n",
    "            # print(f\"resp_pre: {response_pre}\", f\"\\n{'-'*120}\\n\")\n",
    "            return question.__contains__(non_common_substring)\n",
    "        else:\n",
    "            return check_prefix_removed(row, question_col, response_pre_col, response_col, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the check function to both original and perturbed columns\n",
    "original_check = data.apply(\n",
    "    lambda row: check_prefix_removed(\n",
    "        row, \"original_question\", \"original_response_pre\", \"original_response\"\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "perturbed_check = data.apply(\n",
    "    lambda row: check_prefix_removed(\n",
    "        row, \"perturbed_question\", \"perturbed_response_pre\", \"perturbed_response\"\n",
    "    ),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if all rows satisfy `original_question + original_response_pre == original_response`\n",
    "original_check.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if all rows satisfy `perturbed_question + perturbed_response_pre == perturbed_response`\n",
    "perturbed_check.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of False for perturbed_response: 0\n"
     ]
    }
   ],
   "source": [
    "perturbed_false_count = len(perturbed_check) - perturbed_check.sum()\n",
    "print(f\"Count of False for perturbed_response: {perturbed_false_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: bool)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturbed_check[perturbed_check == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perturbed_response_pre == perturbed_response: \n",
      "Series([], Name: count, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"perturbed_response_pre == perturbed_response: \\n{perturbed_check[perturbed_check == False].index.to_series().apply(lambda x: data.loc[x, \"perturbed_response_pre\"] == data.loc[x, \"perturbed_response\"]).value_counts()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: count, dtype: int64)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturbed_check[perturbed_check == False].index.to_series().apply(lambda x: data.loc[x, \"model\"]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in perturbed_check[perturbed_check == False].index:\n",
    "    print(\"------ \", data.loc[index, \"perturbed_question\"], \"\\n\")\n",
    "    print(\"------ \", data.loc[index, \"perturbed_response\"], \"\\n\")\n",
    "    print(\"------ \", data.loc[index, \"perturbed_response_pre\"], \"\\n\")\n",
    "    print(data.loc[index, \"perturbed_response_pre\"] == data.loc[index, \"perturbed_response\"], \"\\n\")\n",
    "    print(f\"{'-'*120}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[139]['perturbed_response'].startswith(data.iloc[139]['perturbed_question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['category', 'subcategory', 'original_question',\n",
       "       'original_question_safety', 'original_response',\n",
       "       'original_response_safety', 'perturbation_level', 'perturbation_type',\n",
       "       'perturbation_count', 'perturbed_question', 'perturbed_question_safety',\n",
       "       'model', 'perturbed_response', 'perturbed_response_safety',\n",
       "       'experiment', 'original_response_pre', 'perturbed_response_pre'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_csv(dataset_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from config import envs, credentials\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moderate(model, tokenizer, texts):\n",
    "    try:\n",
    "        output_texts = []\n",
    "        for id, text in enumerate(texts):\n",
    "\n",
    "            chat = [{\"role\": \"user\", \"content\": text}]\n",
    "            input_ids = tokenizer.apply_chat_template(chat, return_tensors=\"pt\").to(\n",
    "                device\n",
    "            )\n",
    "            output = model.generate(\n",
    "                input_ids=input_ids, max_new_tokens=100, pad_token_id=0\n",
    "            )\n",
    "            prompt_len = input_ids.shape[-1]\n",
    "            output_text = tokenizer.decode(\n",
    "                output[0][prompt_len:], skip_special_tokens=True\n",
    "            )\n",
    "            output_texts.append(output_text)\n",
    "        return output_texts\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating text: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86dc64c5618443619472323e7862d74a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf2d87c0f6f45e8b808b9b8a3248d60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e28c893f96403d8a726885bcea068e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tornado.general:SEND Error: Host unreachable\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0489fd4af85d498b9737af7669b3c9c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/860 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b03dc0dbd47c4dbabfb62e9ea77f0728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26cc3f705bfb4f57aca1a39b75f27143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02211c9bb011493b8a221b07c362ea26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f9209b4b004f429e3bd415fe57d98d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-Guard-3-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_id,\n",
    "    token=credentials.HF_TOKEN,\n",
    "    cache_dir=\"/Users/saurabh/AA/convergent/projects/llm-sensitivity/.env/models\",\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_id,\n",
    "    token=credentials.HF_TOKEN,\n",
    "    cache_dir=\"/Users/saurabh/AA/convergent/projects/llm-sensitivity/.env/models\",\n",
    ").to(device)\n",
    "\n",
    "question_col_list = ['original_response_pre', 'perturbed_response_pre']\n",
    "\n",
    "for question_col in question_col_list:\n",
    "    questions = data[question_col].to_list()\n",
    "\n",
    "    output_texts = moderate(model, tokenizer, questions)\n",
    "    print(f\"Storing response in dataframe\")\n",
    "    new_col_name = f\"{question_col}_safety\"\n",
    "    data.insert(\n",
    "        data.columns.get_loc(question_col) + 1,\n",
    "        new_col_name,\n",
    "        output_texts,\n",
    "    )\n",
    "\n",
    "    data.to_csv(dataset_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
