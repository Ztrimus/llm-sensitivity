{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cW-bZItFXPgJ"
      },
      "outputs": [],
      "source": [
        "import nlpaug.augmenter.word as naw\n",
        "from nlpaug.util import Action\n",
        "\n",
        "import random\n",
        "\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "import nltk\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
            "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
            "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n",
            "[nltk_data] Error loading universal_tagset: <urlopen error [SSL:\n",
            "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
            "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n",
            "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
            "[nltk_data]     [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify\n",
            "[nltk_data]     failed: unable to get local issuer certificate\n",
            "[nltk_data]     (_ssl.c:1000)>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
            "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
            "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n",
            "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
            "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
            "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def get_word_count(text):\n",
        "\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  nonPunct = re.compile('.*[A-Za-z0-9].*')  # must contain a letter or digit\n",
        "\n",
        "  filtered = [w for w in tokens if nonPunct.match(w)]\n",
        "\n",
        "  return len(filtered)\n",
        "\n",
        "def synonym_replacement(words, n, stop_words):\n",
        "    fail_count=0\n",
        "    #words = words.split()\n",
        "\n",
        "    new_words = words.copy()\n",
        "    random_word_list = []\n",
        "    for word in words:\n",
        "      if word[1] in [\"NOUN\", \"ADJ\", \"ADV\", \"VERB\"]:\n",
        "        random_word_list.append(word)\n",
        "    #random_word_list = list(set([word for word in words if word not in stop_words]))\n",
        "    random.shuffle(random_word_list)\n",
        "    num_replaced = 0\n",
        "\n",
        "    for random_word in random_word_list:\n",
        "      synonyms = get_synonyms(random_word)\n",
        "\n",
        "      if len(synonyms) >= 1:\n",
        "          synonym = random.choice(list(synonyms))\n",
        "          new_words = [synonym if word == random_word else word[0] for word in words]\n",
        "          num_replaced += 1\n",
        "\n",
        "      if num_replaced >= n: #only replace up to n words\n",
        "          break\n",
        "    try:\n",
        "      sentence = ' '.join(new_words)\n",
        "    except TypeError as e:\n",
        "      print(e)\n",
        "      print(new_words)\n",
        "      old_words = [word[0] for word in words]\n",
        "      sentence = ' '.join(old_words)\n",
        "      fail_count+=1\n",
        "\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def get_synonyms(word):\n",
        "    \"\"\"\n",
        "    Get synonyms of a word\n",
        "    \"\"\"\n",
        "    synonyms = set()\n",
        "    if word[1]==\"NOUN\":\n",
        "      param = wn.NOUN\n",
        "    elif word[1]==\"VERB\":\n",
        "      param = wn.VERB\n",
        "    elif word[1]==\"ADV\":\n",
        "      param=wn.ADV\n",
        "    elif word[1]==\"ADJ\":\n",
        "      param=wn.ADJ\n",
        "    else:\n",
        "      ## do something TODO\n",
        "      return []\n",
        "    for syn in wn.synsets(word[0], pos=param):\n",
        "        for l in syn.lemmas():\n",
        "            synonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
        "            synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
        "            synonyms.add(synonym)\n",
        "\n",
        "\n",
        "    if word[0] in synonyms:\n",
        "        synonyms.remove(word[0])\n",
        "    #print(\"INSIDE GET_SYNONYMS: \")\n",
        "    #print(synonyms)\n",
        "    return list(synonyms)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXeIVCDIGDci"
      },
      "outputs": [],
      "source": [
        "def augment_dataset(data_frame, output_filepath, percentage=0.2):\n",
        "\n",
        "  augmented_dataset = []\n",
        "\n",
        "  for article in tqdm(data_frame.itertuples()):\n",
        "\n",
        "    body_texts = article.text\n",
        "\n",
        "    # body_texts = body_texts.split(\".\")\n",
        "\n",
        "    ## --- temporary ---\n",
        "\n",
        "    sr_article = article.text_perturb\n",
        "\n",
        "    ## --- temporary ---\n",
        "\n",
        "    # new_sent = []\n",
        "\n",
        "\n",
        "    ## synonym replacement:\n",
        "\n",
        "    # for sent in body_texts:\n",
        "\n",
        "    #   word_count = get_word_count(sent)\n",
        "    #   n = int(word_count*percentage)\n",
        "    #   sent_tokens = word_tokenize(sent)\n",
        "    #   sent_with_pos = nltk.pos_tag(sent_tokens, tagset='universal')\n",
        "    #   new_sent.append(synonym_replacement(sent_with_pos, n, stop_words))\n",
        "\n",
        "    # sr_article = \". \".join(new_sent)\n",
        "\n",
        "\n",
        "    ## Random Swap of aug_p% or fraction of words\n",
        "\n",
        "    aug = naw.RandomWordAug(action=\"swap\", aug_p=0.3)\n",
        "    random_swap = aug.augment(body_texts)\n",
        "\n",
        "    ## Random Deletion of aug_p% or fraction of words\n",
        "\n",
        "\n",
        "    aug = naw.RandomWordAug(action=\"delete\", aug_max=None, aug_p=0.3)\n",
        "    random_del = aug.augment(body_texts)\n",
        "\n",
        "\n",
        "    ## Random Crop of aug_p fraction length span\n",
        "\n",
        "\n",
        "    aug = naw.RandomWordAug(action=\"crop\", aug_max=None, aug_p=0.3)\n",
        "    try:\n",
        "      random_crop = aug.augment(body_texts)\n",
        "    except ValueError as e:\n",
        "      print(e)\n",
        "      continue\n",
        "\n",
        "    ## Random spelling mistakes\n",
        "\n",
        "    aug = naw.SpellingAug(aug_max=None, aug_p=0.3)\n",
        "    random_spelling = aug.augment(body_texts)\n",
        "\n",
        "    try:\n",
        "      augmented_dataset.append([article.text, sr_article, random_swap[0], random_del[0], random_crop[0],\n",
        "                                random_spelling[0], article.label])\n",
        "    except IndexError as e:\n",
        "      print(e)\n",
        "      print(\"original\")\n",
        "      print(body_texts)\n",
        "      print(random_swap, random_del, random_crop, random_spelling)\n",
        "      continue\n",
        "\n",
        "    # break\n",
        "  augmented_frame = pd.DataFrame(augmented_dataset, columns=[\"original\",\"synonym_replacement\", \"random_swap\",\n",
        "                                                             \"random_delete\", \"random_crop\", \"random_spelling\", \"label\"])\n",
        "\n",
        "  jsonl_data = augmented_frame.to_json(orient='records', lines=True)\n",
        "\n",
        "  with open(output_filepath, \"w\") as text_file:\n",
        "    text_file.write(jsonl_data)\n",
        "\n",
        "  return augmented_frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rp11LkozXMZP"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/Shareddrives/DARPA/Data Perturbations/Synonym_Replacement/\"\n",
        "\n",
        "real_train = pd.read_json(path + \"neural_news_real.train.jsonl\", lines=True, orient=\"records\")\n",
        "real_test = pd.read_json(path + \"neural_news_real.test.jsonl\", lines=True, orient=\"records\")\n",
        "real_holdout = pd.read_json(path + \"neural_news_real.holdout.jsonl\", lines=True, orient=\"records\")\n",
        "\n",
        "fake_train = pd.read_json(path + \"neural_news_fake.train.jsonl\", lines=True, orient=\"records\")\n",
        "fake_test = pd.read_json(path + \"neural_news_fake.test.jsonl\", lines=True, orient=\"records\")\n",
        "fake_holdout = pd.read_json(path + \"neural_news_fake.holdout.jsonl\", lines=True, orient=\"records\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6ofz3INSIpc"
      },
      "outputs": [],
      "source": [
        "augment_dataset(real_train, output_filepath=path+\"five_augs.neural_news_real.train.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcM0QzobWoFt"
      },
      "outputs": [],
      "source": [
        "augment_dataset(real_test, output_filepath=path+\"five_augs.neural_news_real.test.jsonl\")\n",
        "\n",
        "augment_dataset(fake_train, output_filepath=path+\"five_augs.neural_news_fake.train.jsonl\")\n",
        "augment_dataset(fake_test, output_filepath=path+\"five_augs.neural_news_fake.test.jsonl\")\n",
        "augment_dataset(fake_holdout, output_filepath=path+\"five_augs.neural_news_fake.holdout.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELsSMbcdccEy"
      },
      "outputs": [],
      "source": [
        "augment_dataset(real_holdout, output_filepath=path+\"five_augs.neural_news_real.holdout.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLlLRR6VZu8Z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
