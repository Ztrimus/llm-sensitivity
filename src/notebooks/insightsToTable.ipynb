{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the directory containing the datasets\n",
    "data_dir = \"../../data/\"\n",
    "dataset_path = os.path.join(data_dir, \"analyzed/catHarmQA/combined_catqa.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns with \"_pre\" in it's name, means questions has been removed from response text. having question in response affect safe of response. that's why question should remove from response.\n",
    "data.original_response_safety = data.original_response_pre_safety\n",
    "data.perturbed_response_safety = data.perturbed_response_pre_safety\n",
    "data.original_response = data.original_response_pre\n",
    "data.perturbed_response = data.perturbed_response_pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Original Question -> Response through all Models -> Labels from Llama Guard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of safe and unsafe responses of each model for original questions\n",
    "data.groupby(\"model\")[\"original_response_safety\"].value_counts(normalize=True).unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Original Question -> filter out Unsafe Original Question using LLama-Guard -> Get Response on filtered question from all Models -> Get Labels from Llama Guard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.original_question_safety.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered unsafe original question according to llama guard\n",
    "data[data[\"original_question_safety\"] == \"unsafe\"].groupby(\"model\")[\"original_response_safety\"].value_counts(normalize=True).unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Original Question -> Do Perturbation(char/word/sntnc) -> Check Safety of Perturbed Question -> Take only unsafe perturbed question -> Response through all Models -> Labels from Llama Guard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Safe percetnage of char, word, sentnc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safety percentages for perturbed questions\n",
    "data[data[\"perturbed_question_safety\"] == \"unsafe\"].groupby(\n",
    "    \"perturbation_level\"\n",
    "    ).perturbed_response_safety.value_counts(normalize=True).unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. re-verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level in [\"char\",\"word\", \"sntnc\"]:\n",
    "    dd = data[data.perturbation_level == level]\n",
    "    dd = dd[dd.perturbed_question_safety == \"unsafe\"]\n",
    "    dd = dd.perturbed_response_safety.value_counts(normalize=True)*100\n",
    "    print(f\"{level} : {dd.safe:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Unsafe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data[\"perturbed_question_safety\"] == \"unsafe\"].groupby([\"model\" , \"perturbation_level\"])[\"perturbed_response_safety\"].value_counts(normalize=True).unstack() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display options\n",
    "pd.set_option(\"display.max_rows\", None)  # Show all rows\n",
    "pd.set_option(\"display.max_columns\", None)  # Show all columns\n",
    "pd.set_option(\"display.width\", None)  # Do not wrap columns\n",
    "pd.set_option(\"display.max_colwidth\", None)  # Do not truncate column values\n",
    "\n",
    "data[data[\"perturbed_question_safety\"] == \"unsafe\"].groupby(\n",
    "    [\"model\", \"perturbation_level\", \"category\"]\n",
    ")[\"perturbed_response_safety\"].value_counts(normalize=True).unstack() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data[\"perturbed_question_safety\"] == \"unsafe\"].groupby(\n",
    "    [\"perturbation_level\" , \"category\"]\n",
    ")[\"perturbed_response_safety\"].value_counts(normalize=True).unstack().T * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
