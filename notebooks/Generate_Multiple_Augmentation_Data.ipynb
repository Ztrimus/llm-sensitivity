{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rsgb7gBbJZw6",
        "outputId": "8e27da14-a1b2-4473-93e3-505edbc60268"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy requests nlpaug\n",
        "!pip install torch>=1.6.0 transformers>=4.11.3 sentencepiece"
      ],
      "metadata": {
        "id": "Kl5qgAGEXRrE",
        "outputId": "3da7bcd6-362e-4ca5-f952-3185e7914170",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Collecting nlpaug\n",
            "  Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
            "\u001b[K     |████████████████████████████████| 410 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from nlpaug) (4.4.0)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from nlpaug) (1.3.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown>=4.0.0->nlpaug) (4.64.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown>=4.0.0->nlpaug) (4.6.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown>=4.0.0->nlpaug) (1.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown>=4.0.0->nlpaug) (3.8.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->nlpaug) (2022.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests) (1.7.1)\n",
            "Installing collected packages: nlpaug\n",
            "Successfully installed nlpaug-1.1.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nlpaug.augmenter.word as naw\n",
        "from nlpaug.util import Action\n",
        "\n",
        "import random"
      ],
      "metadata": {
        "id": "cW-bZItFXPgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jsSS8TciQSz",
        "outputId": "145e1aad-8881-45f7-8675-52b59793b3dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "import random\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "def get_word_count(text):\n",
        "\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  nonPunct = re.compile('.*[A-Za-z0-9].*')  # must contain a letter or digit\n",
        "\n",
        "  filtered = [w for w in tokens if nonPunct.match(w)]\n",
        "\n",
        "  return len(filtered)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def synonym_replacement(words, n, stop_words):\n",
        "    fail_count=0\n",
        "    #words = words.split()\n",
        "\n",
        "    new_words = words.copy()\n",
        "    random_word_list = []\n",
        "    for word in words:\n",
        "      if word[1] in [\"NOUN\", \"ADJ\", \"ADV\", \"VERB\"]:\n",
        "        random_word_list.append(word)\n",
        "    #random_word_list = list(set([word for word in words if word not in stop_words]))\n",
        "    random.shuffle(random_word_list)\n",
        "    num_replaced = 0\n",
        "\n",
        "    for random_word in random_word_list:\n",
        "      synonyms = get_synonyms(random_word)\n",
        "\n",
        "      if len(synonyms) >= 1:\n",
        "          synonym = random.choice(list(synonyms))\n",
        "          new_words = [synonym if word == random_word else word[0] for word in words]\n",
        "          num_replaced += 1\n",
        "\n",
        "      if num_replaced >= n: #only replace up to n words\n",
        "          break\n",
        "    try:\n",
        "      sentence = ' '.join(new_words)\n",
        "    except TypeError as e:\n",
        "      print(e)\n",
        "      print(new_words)\n",
        "      old_words = [word[0] for word in words]\n",
        "      sentence = ' '.join(old_words)\n",
        "      fail_count+=1\n",
        "\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def get_synonyms(word):\n",
        "    \"\"\"\n",
        "    Get synonyms of a word\n",
        "    \"\"\"\n",
        "    synonyms = set()\n",
        "    if word[1]==\"NOUN\":\n",
        "      param = wn.NOUN\n",
        "    elif word[1]==\"VERB\":\n",
        "      param = wn.VERB\n",
        "    elif word[1]==\"ADV\":\n",
        "      param=wn.ADV\n",
        "    elif word[1]==\"ADJ\":\n",
        "      param=wn.ADJ\n",
        "    else:\n",
        "      ## do something TODO\n",
        "      return []\n",
        "    for syn in wn.synsets(word[0], pos=param):\n",
        "        for l in syn.lemmas():\n",
        "            synonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
        "            synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
        "            synonyms.add(synonym)\n",
        "\n",
        "\n",
        "    if word[0] in synonyms:\n",
        "        synonyms.remove(word[0])\n",
        "    #print(\"INSIDE GET_SYNONYMS: \")\n",
        "    #print(synonyms)\n",
        "    return list(synonyms)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_dataset(data_frame, output_filepath, percentage=0.2):\n",
        "\n",
        "  augmented_dataset = []\n",
        "\n",
        "  for article in tqdm(data_frame.itertuples()):\n",
        "\n",
        "    body_texts = article.text\n",
        "\n",
        "    # body_texts = body_texts.split(\".\")\n",
        "\n",
        "    ## --- temporary ---\n",
        "\n",
        "    sr_article = article.text_perturb\n",
        "\n",
        "    ## --- temporary ---\n",
        "\n",
        "    # new_sent = []\n",
        "\n",
        "\n",
        "    ## synonym replacement:\n",
        "\n",
        "    # for sent in body_texts:\n",
        "\n",
        "    #   word_count = get_word_count(sent)\n",
        "    #   n = int(word_count*percentage)\n",
        "    #   sent_tokens = word_tokenize(sent)\n",
        "    #   sent_with_pos = nltk.pos_tag(sent_tokens, tagset='universal')\n",
        "    #   new_sent.append(synonym_replacement(sent_with_pos, n, stop_words))\n",
        "\n",
        "    # sr_article = \". \".join(new_sent)\n",
        "\n",
        "\n",
        "    ## Random Swap of aug_p% or fraction of words\n",
        "\n",
        "    aug = naw.RandomWordAug(action=\"swap\", aug_p=0.3)\n",
        "    random_swap = aug.augment(body_texts)\n",
        "\n",
        "    ## Random Deletion of aug_p% or fraction of words\n",
        "\n",
        "\n",
        "    aug = naw.RandomWordAug(action=\"delete\", aug_max=None, aug_p=0.3)\n",
        "    random_del = aug.augment(body_texts)\n",
        "\n",
        "\n",
        "    ## Random Crop of aug_p fraction length span\n",
        "\n",
        "\n",
        "    aug = naw.RandomWordAug(action=\"crop\", aug_max=None, aug_p=0.3)\n",
        "    try:\n",
        "      random_crop = aug.augment(body_texts)\n",
        "    except ValueError as e:\n",
        "      print(e)\n",
        "      continue\n",
        "\n",
        "    ## Random spelling mistakes\n",
        "\n",
        "    aug = naw.SpellingAug(aug_max=None, aug_p=0.3)\n",
        "    random_spelling = aug.augment(body_texts)\n",
        "\n",
        "    try:\n",
        "      augmented_dataset.append([article.text, sr_article, random_swap[0], random_del[0], random_crop[0],\n",
        "                                random_spelling[0], article.label])\n",
        "    except IndexError as e:\n",
        "      print(e)\n",
        "      print(\"original\")\n",
        "      print(body_texts)\n",
        "      print(random_swap, random_del, random_crop, random_spelling)\n",
        "      continue\n",
        "\n",
        "    # break\n",
        "  augmented_frame = pd.DataFrame(augmented_dataset, columns=[\"original\",\"synonym_replacement\", \"random_swap\",\n",
        "                                                             \"random_delete\", \"random_crop\", \"random_spelling\", \"label\"])\n",
        "\n",
        "  jsonl_data = augmented_frame.to_json(orient='records', lines=True)\n",
        "\n",
        "  with open(output_filepath, \"w\") as text_file:\n",
        "    text_file.write(jsonl_data)\n",
        "\n",
        "  return augmented_frame"
      ],
      "metadata": {
        "id": "sXeIVCDIGDci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/Shareddrives/DARPA/Data Perturbations/Synonym_Replacement/\"\n",
        "\n",
        "real_train = pd.read_json(path + \"neural_news_real.train.jsonl\", lines=True, orient=\"records\")\n",
        "real_test = pd.read_json(path + \"neural_news_real.test.jsonl\", lines=True, orient=\"records\")\n",
        "real_holdout = pd.read_json(path + \"neural_news_real.holdout.jsonl\", lines=True, orient=\"records\")\n",
        "\n",
        "fake_train = pd.read_json(path + \"neural_news_fake.train.jsonl\", lines=True, orient=\"records\")\n",
        "fake_test = pd.read_json(path + \"neural_news_fake.test.jsonl\", lines=True, orient=\"records\")\n",
        "fake_holdout = pd.read_json(path + \"neural_news_fake.holdout.jsonl\", lines=True, orient=\"records\")"
      ],
      "metadata": {
        "id": "rp11LkozXMZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augment_dataset(real_train, output_filepath=path+\"five_augs.neural_news_real.train.jsonl\")"
      ],
      "metadata": {
        "id": "T6ofz3INSIpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augment_dataset(real_test, output_filepath=path+\"five_augs.neural_news_real.test.jsonl\")\n",
        "\n",
        "augment_dataset(fake_train, output_filepath=path+\"five_augs.neural_news_fake.train.jsonl\")\n",
        "augment_dataset(fake_test, output_filepath=path+\"five_augs.neural_news_fake.test.jsonl\")\n",
        "augment_dataset(fake_holdout, output_filepath=path+\"five_augs.neural_news_fake.holdout.jsonl\")"
      ],
      "metadata": {
        "id": "NcM0QzobWoFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augment_dataset(real_holdout, output_filepath=path+\"five_augs.neural_news_real.holdout.jsonl\")"
      ],
      "metadata": {
        "id": "ELsSMbcdccEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PLlLRR6VZu8Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}