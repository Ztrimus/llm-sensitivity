{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Perturbations \n",
    "Perturbations are intentional modifications made to data to simulate variations, errors, or noise, aiming to test or improve the robustness and adaptability of models to real-world, imperfect inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1.1. Types of Perturbations\n",
    "\n",
    "Mistakes Humans Make While Interacting with LLMs:\n",
    "1. **Grammatical Mistakes**: Incorrect use of tense, prepositions, article omission or misuse, verb agreement errors, and sentence fragment errors.\n",
    "2. **Spelling Errors**: Typos, homophones (words that sound alike but have different meanings), and commonly confused words (e.g., \"there\" vs. \"their\").\n",
    "3. **Unclear Task Definitions**: Vague questions, incomplete thoughts, or requests that lack specific details necessary for clear understanding.\n",
    "4. **Use of Slang or Informal Language**: Colloquial expressions, regional dialects, internet slang, and abbreviations that may not be universally understood or are too casual for formal contexts.\n",
    "6. **Non-standard Syntax**: Inversion of the normal word order, overly complex sentences, or sentences that lack the usual structure.\n",
    "5. **Overly Complex or Compound Queries**: Questions that contain multiple parts or topics, which may not be clearly delineated.\n",
    "7. **Ambiguity in Queries**: Questions that can be interpreted in multiple ways due to vague wording or lack of context.\n",
    "8. **Use of Jargon or Highly Technical Language**: Specific to certain fields or interests, which might not be universally understood.\n",
    "9. **Assumptions of Common Knowledge**: Questions that assume a base level of knowledge or context that the LLM might not have.\n",
    "10. **Cultural References or Idioms**: Phrases or references that rely on cultural knowledge not shared by all users or the LLM.\n",
    "11. **Inconsistent Formatting**: Mixed use of date formats, currency, units of measurement, or switching between first, second, and third person.\n",
    "12. **Omission of Crucial Information**: Leaving out key details necessary to understand the request or assuming the model has access to external information not provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: (Optional) Find resources "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1.2. Techniques for Introducing Perturbations in Text/Questions/Dataset:\n",
    "\n",
    "1. **Rule-Based Error Injection**: Manually define rules for introducing specific types of errors:\n",
    "    - *Sentence Scrambling*: Rearrange the order of words or phrases in a sentence to simulate syntactic errors or unclear task definitions.\n",
    "    - *Homophone Replacement*: Replace words with their homophones to simulate common spelling errors.\n",
    "    - *Omission Simulation*: Randomly omit crucial words or phrases to simulate the omission of information.\n",
    "    - Informal Language Injection: Introduce slang or informal phrases into sentences to simulate casual language use.\n",
    "    - etc.\n",
    "2. **Noise Injection**: To mimic spelling and grammatical errors:\n",
    "    - *Synonym Replacement*: Randomly replace words with their synonyms to simulate the varied vocabulary humans might use. This can also introduce informal language variations.\n",
    "    - Randomly alter characters\n",
    "    - add words\n",
    "    - delete words\n",
    "    - etc.\n",
    "3. **Back Translation**: Translate the text to another language and then back to the original language. This often introduces grammatical inaccuracies and changes in sentence structure.\n",
    "4. **Paraphrasing Tools**: Use AI-based paraphrasing tools to rewrite questions in ways that might introduce ambiguity or complexity.\n",
    "5. **Text Simplification/Complexification**: Use models to either simplify or make the text more complex, aiming to mimic the way different users might phrase the same question.\n",
    "6. **Synthetic Data Generation**: Leverage generative models to create new questions based on the original ones, introducing errors or variations in the process.\n",
    "7. **Keyboard Typing Errors**: Introduce errors based on common typing mistakes, considering keyboard layout (e.g., letters often mistyped because of their proximity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementation in Python\n",
    "1. **Rule-Based Error Injection**: Implement functions to drop random articles, misuse verbs, and introduce common spelling errors.\n",
    "2. **Noise Injection**: Create a function to randomly replace characters or words in sentences, mimicking typographical errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "import nltk\n",
    "\n",
    "# Ensure that NLTK resources are downloaded (if necessary)\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perturbation:\n",
    "    def __init__(self, question):\n",
    "        self.question = question\n",
    "        self.homophones = {\n",
    "            \"capital\": \"capitol\",\n",
    "            \"sea\": \"see\",\n",
    "            \"right\": \"write\",\n",
    "            \"France\": \"french\"\n",
    "            # TODO: Find corpus for homphones from NLTK or other sources\n",
    "        } \n",
    "    \n",
    "    #### 2.1. Rule-Based Error Injection ####\n",
    "    def sentence_scrambling(self):\n",
    "        \"\"\"Sentence Scrambling\"\"\"\n",
    "        words = self.question.split()\n",
    "        random.shuffle(words)\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def homophone_replacement(self):\n",
    "        \"\"\"Homophone Replacement\"\"\"\n",
    "        words = self.question.split()\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            # Checking if the word's lowercase is in the homophones dictionary\n",
    "            key = word.lower()\n",
    "            if key in self.homophones:\n",
    "                # Replace with homophone, preserving the original case\n",
    "                homophone = self.homophones[key]\n",
    "                if word.istitle():\n",
    "                    homophone = homophone.capitalize()\n",
    "                new_words.append(homophone)\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "        self.question = ' '.join(new_words)\n",
    "        return self.question\n",
    "    \n",
    "    def omission(self):\n",
    "        \"\"\"Omission\"\"\"\n",
    "        words = self.question.split()\n",
    "        # TODO: Improve the logic to omit words more dynamically, while preserving meaning as much as possible\n",
    "        omissible_words = [word for word in words if word.lower() not in ['how', 'what', 'why', 'who', 'where', '?']]\n",
    "        if omissible_words:\n",
    "            omitted_word = random.choice(omissible_words)\n",
    "            words.remove(omitted_word)\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def informal_language_injection(self):\n",
    "        \"\"\"Introduces slang or informal phrases into sentences\"\"\"\n",
    "        # TODO: This is a basic implementation, and it could be expanded with a more comprehensive mapping or logic.\n",
    "        informal_replacements = {\n",
    "            \"What is\": \"What's\",\n",
    "            \"How many\": \"How many\",\n",
    "            \"Why is\": \"Why's\",\n",
    "            \"Who is\": \"Who's\",\n",
    "            # Add more replacements as needed\n",
    "        }\n",
    "        for formal, informal in informal_replacements.items():\n",
    "            self.question = self.question.replace(formal, informal)\n",
    "        return self.question\n",
    "    \n",
    "    #### 2.2. Noise Injection ####\n",
    "    def synonym_replacement(self, n=1):\n",
    "        \"\"\"Replace up to n words with their synonyms\"\"\"\n",
    "        words = self.question.split()\n",
    "        new_sentence = []\n",
    "        replacements = 0\n",
    "\n",
    "        for word in words:\n",
    "            synonyms = [lem.name() for syn in wordnet.synsets(word) for lem in syn.lemmas() if lem.name() != word]\n",
    "            if synonyms and replacements < n:\n",
    "                new_sentence.append(random.choice(synonyms))\n",
    "                replacements += 1\n",
    "            else:\n",
    "                new_sentence.append(word)\n",
    "        \n",
    "        return ' '.join(new_sentence)\n",
    "\n",
    "    def random_character_alteration(self, n=1):\n",
    "        \"\"\"Randomly alter characters in the question\"\"\"\n",
    "        chars = list(self.question)\n",
    "        for _ in range(n):\n",
    "            index = random.randint(0, len(chars) - 1)\n",
    "            alteration_type = random.choice(['insert', 'delete', 'substitute'])\n",
    "            if alteration_type == 'insert':\n",
    "                chars.insert(index, random.choice('abcdefghijklmnopqrstuvwxyz'))\n",
    "            elif alteration_type == 'delete' and len(chars) > 1:\n",
    "                del chars[index]\n",
    "            elif alteration_type == 'substitute':\n",
    "                chars[index] = random.choice('abcdefghijklmnopqrstuvwxyz')\n",
    "        return ''.join(chars)\n",
    "\n",
    "    def random_word_addition_deletion(self, add=True, delete=True):\n",
    "        \"\"\"Randomly add or delete words from the sentence\"\"\"\n",
    "        words = self.question.split()\n",
    "        if add:\n",
    "            # Add a random word\n",
    "            insert_at = random.randint(0, len(words))\n",
    "            words.insert(insert_at, random.choice(words))\n",
    "        if delete and len(words) > 1:\n",
    "            # Delete a random word\n",
    "            del words[random.randint(0, len(words) - 1)]\n",
    "        return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_questions = ['How many Dubai-based airlines are owned by Somalis?',\n",
    "       'What product did various compounds of zinc show massive differences in absorption?',\n",
    "       'What do athletes perform for tumbling?',\n",
    "       'Who plays an influential role in the formation of party policy?',\n",
    "       \"How many thumbs down votes did youtube's official statement about the new commenting system get within two days?\",\n",
    "       'In what year were the Beer Orders passed?',\n",
    "       'What did the United States of America incorporate?',\n",
    "       'What two letters can be replaced with each other a lot of the time in Estonian?',\n",
    "       \"What isn't the threshold of the number of copies and the value of the works?\",\n",
    "       \"Who received the Judges' Save this season?\",\n",
    "       'What is the cost to build Cornell Tech?',\n",
    "       'On what day did Paul VI die?',\n",
    "       'Why is there no saving grace of relying on these theories?',\n",
    "       'How did the Jewish population increase before the 1st century?',\n",
    "       'What was the former name of the Tampa Bay Storm?',\n",
    "       'Why was Shtokavian the most widespread culture in the western Balkans? ',\n",
    "       'How many commercial airports does Fraport in the UK manage?',\n",
    "       'What quality has LEDs been used as?',\n",
    "       'What area was the likely focal point of the Roman senate?',\n",
    "       'When was the Tower constructed?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply perturbations to sample questions\n",
    "perturbation = Perturbation(random.choice(sample_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jewish did century? 1st before increase the the How population'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturbation.sentence_scrambling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How did the Jewish population increase before the 1st century?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturbation.question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Back Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: For Back Translation, use very famous language Hindi, Marathi, Bengali, Mandarin, Spanish, French, Arabic, Russian, Portuguese, German, Japanese, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Sample sentence from SQuAD\n",
    "sentence = \"What is the capital of France?\"\n",
    "\n",
    "# Define translation pipelines\n",
    "translate_to_french = pipeline('translation_en_to_fr')\n",
    "translate_back_to_english = pipeline('translation_fr_to_en')\n",
    "\n",
    "# Perform back-translation\n",
    "translated_to_french = translate_to_french(sentence)[0]['translation_text']\n",
    "back_translated_to_english = translate_back_to_english(translated_to_french)[0]['translation_text']\n",
    "\n",
    "print(\"Original:\", sentence)\n",
    "print(\"Back Translated:\", back_translated_to_english)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Keyboard Typing Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: For Keyboard Typing Errors, Create 5X12 matrix with characters mentioned on keyboard accoridng to their position. \n",
    "# Then to create typo replace any character with its adjacent character according to keyboard position.\n",
    "import numpy as np\n",
    "\n",
    "keyboard = np.array([\n",
    "    list('`1234567890-= '),\n",
    "    list('\\tqwertyuiop[]\\\\'),\n",
    "    list('  asdfghjkl;\\' '),\n",
    "    list('  zxcvbnm,./  '),\n",
    "    list('              ')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: For Parapharasing, Can we use writer's style of writing?\n",
    "# how would following author paraphrase the given sentence?\n",
    "# 1. Paulo Coelho\n",
    "# 2. Stephen King\n",
    "# 3. Agatha Christie\n",
    "# 4. J.K. Rowling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
